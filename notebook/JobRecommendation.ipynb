{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Job Recommendation System**\n",
        "\n",
        "**Step 1:** Install Required Packages\n",
        "This system requires several Python packages to function properly. We install:\n",
        "\n",
        "sentence-transformers: For generating text embeddings that capture semantic meaning of resumes and job descriptions\n",
        "\n",
        "umap-learn: For dimensionality reduction to visualize high-dimensional embedding spaces\n",
        "\n",
        "kagglehub: For downloading the resume dataset directly from Kaggle"
      ],
      "metadata": {
        "id": "P1XlYE-bTlqK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQxMzv6ZYCIW"
      },
      "outputs": [],
      "source": [
        "# JOB RECOMMENDATION SYSTEM\n",
        "\n",
        "# STEP 1: INSTALL REQUIRED PACKAGES\n",
        "\n",
        "!pip install sentence-transformers --quiet\n",
        "!pip install umap-learn --quiet\n",
        "!pip install kagglehub --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Import All Libraries\n",
        "We import all necessary Python libraries for data processing, machine learning, and visualization. This consolidated approach ensures all dependencies are loaded before any code execution begins."
      ],
      "metadata": {
        "id": "ArZvNU_vWprw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDg8rlpbfpsQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# STEP 2: IMPORT ALL LIBRARIES\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# ML/NLP Libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize\n",
        "import umap\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Google Colab integration\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Load Resume Dataset\n",
        "We download and load the resume dataset from Kaggle, which contains resumes in multiple formats (text and HTML) along with categories. Since Kaggle datasets can be stored in different locations depending on the environment, we try multiple possible file paths to ensure successful loading across different platforms."
      ],
      "metadata": {
        "id": "8-1DZWSWW14d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F-oAku8jpIt"
      },
      "outputs": [],
      "source": [
        "# STEP 3: LOAD RESUME DATASET\n",
        "# Description: Load resume dataset from Kaggle using kagglehub\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download and load resume dataset\n",
        "path = kagglehub.dataset_download(\"snehaanbhawal/resume-dataset\")\n",
        "\n",
        "# Try multiple possible paths\n",
        "paths = [\n",
        "    \"/kaggle/input/resume-dataset/Resume/Resume.csv\",\n",
        "    \"/root/.cache/kagglehub/datasets/snehaanbhawal/resume-dataset/versions/1/Resume/Resume.csv\"\n",
        "]\n",
        "\n",
        "for p in paths:\n",
        "    try:\n",
        "        df = pd.read_csv(p, encoding=\"latin-1\")\n",
        "        print(\"Successfully loaded:\", p)\n",
        "        break\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:** Mount Google Drive\n",
        "To access the ONET dataset (which contains comprehensive occupational information), we need to mount Google Drive. The ONET dataset should be stored in the drive under an 'ONET' directory. We mount the drive and list its contents to verify the directory structure."
      ],
      "metadata": {
        "id": "vp6MyrsoW7uN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEBOVbrJjk45"
      },
      "outputs": [],
      "source": [
        "# STEP 4: MOUNT GOOGLE DRIVE\n",
        "# Description: Mount Google Drive to access O*NET dataset\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(os.listdir('/content/drive/MyDrive'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5:** Load O*NET Dataset\n",
        "\n",
        "The O*NET (Occupational Information Network) dataset provides detailed information about occupations, including required skills, tasks, work activities, and more. We load multiple files from this dataset, each providing different dimensions of job data. The dataset uses tab-separated values and Latin-1 encoding."
      ],
      "metadata": {
        "id": "tFb7eo4wXHnI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-NWJ3twkNMV"
      },
      "outputs": [],
      "source": [
        "# STEP 5: LOAD O*NET DATASET\n",
        "# Description: Load various O*NET files for occupation data\n",
        "\n",
        "# Check if ONET directory exists\n",
        "try:\n",
        "    print(os.listdir('/content/drive/MyDrive/ONET'))\n",
        "except FileNotFoundError:\n",
        "    print(\"ONET directory not found. Please ensure O*NET dataset is in your Google Drive.\")\n",
        "\n",
        "# Load O*NET files\n",
        "skills = pd.read_csv('/content/drive/MyDrive/ONET/Skills.txt', sep='\\t', encoding='latin-1')\n",
        "abilities = pd.read_csv('/content/drive/MyDrive/ONET/Abilities.txt', sep='\\t', encoding='latin-1')\n",
        "tasks = pd.read_csv('/content/drive/MyDrive/ONET/Task Statements.txt', sep='\\t', encoding='latin-1')\n",
        "occupation = pd.read_csv('/content/drive/MyDrive/ONET/Occupation Data.txt', sep='\\t', encoding='latin-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Create Occupation Profiles**\n",
        "\n",
        "We combine multiple O*NET files into comprehensive occupation profiles. Each profile includes basic information (title and description), plus aggregated lists of required skills, typical tasks, and work activities. This structured format makes it easier to compare occupations and match them with resumes."
      ],
      "metadata": {
        "id": "1mVv3x39XMVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLQcvNV6l0gX"
      },
      "outputs": [],
      "source": [
        "# STEP 6: CREATE OCCUPATION PROFILES\n",
        "# Description: Combine multiple O*NET files into comprehensive occupation profiles\n",
        "\n",
        "# Loading key ONET files\n",
        "occ = pd.read_csv('/content/drive/MyDrive/ONET/Occupation Data.txt', sep='\\t', encoding='latin-1')\n",
        "skills = pd.read_csv('/content/drive/MyDrive/ONET/Skills.txt', sep='\\t', encoding='latin-1')\n",
        "tasks = pd.read_csv('/content/drive/MyDrive/ONET/Task Statements.txt', sep='\\t', encoding='latin-1')\n",
        "activities = pd.read_csv('/content/drive/MyDrive/ONET/Work Activities.txt', sep='\\t', encoding='latin-1')\n",
        "\n",
        "# Group aggregation (join multiple rows per occupation)\n",
        "occ_skills = skills.groupby('O*NET-SOC Code')['Element Name'].apply(list).reset_index()\n",
        "occ_tasks = tasks.groupby('O*NET-SOC Code')['Task'].apply(list).reset_index()\n",
        "occ_activities = activities.groupby('O*NET-SOC Code')['Element Name'].apply(list).reset_index()\n",
        "\n",
        "# Merge them all\n",
        "occupation_profile = occ[['O*NET-SOC Code', 'Title', 'Description']] \\\n",
        "    .merge(occ_skills, on='O*NET-SOC Code', how='left') \\\n",
        "    .merge(occ_tasks, on='O*NET-SOC Code', how='left') \\\n",
        "    .merge(occ_activities, on='O*NET-SOC Code', how='left')\n",
        "\n",
        "occupation_profile.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Prepare Embeddings**\n",
        "\n",
        "Text embeddings convert textual information into numerical vectors that capture semantic meaning. We clean the data, combine multiple text fields from each occupation profile, and use a pre-trained language model (all-MiniLM-L6-v2) to generate embeddings. These embeddings will allow us to compute similarity between resumes and job descriptions."
      ],
      "metadata": {
        "id": "A9NT-A82XZvC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ANVjEK3okWwS"
      },
      "outputs": [],
      "source": [
        "# STEP 7: PREPARE EMBEDDINGS\n",
        "# Description: Clean and prepare text data for embedding generation\n",
        "\n",
        "print(occupation_profile.columns)\n",
        "occupation_profile = occupation_profile.rename(columns={\n",
        "    'Element Name_x': 'Skills',\n",
        "    'Task': 'Tasks',\n",
        "    'Element Name_y': 'Work_Activities'\n",
        "})\n",
        "\n",
        "# Remove duplicates in lists\n",
        "for col in ['Skills', 'Tasks', 'Work_Activities']:\n",
        "    occupation_profile[col] = occupation_profile[col].apply(\n",
        "        lambda x: list(set(x)) if isinstance(x, list) else x\n",
        "    )\n",
        "\n",
        "# Combine text fields\n",
        "def combine_text(row):\n",
        "    text = f\"{row['Title']} {row['Description']} \"\n",
        "    if isinstance(row['Skills'], list):\n",
        "        text += ' '.join(row['Skills'])\n",
        "    if isinstance(row['Tasks'], list):\n",
        "        text += ' '.join(row['Tasks'])\n",
        "    if isinstance(row['Work_Activities'], list):\n",
        "        text += ' '.join(row['Work_Activities'])\n",
        "    return text\n",
        "\n",
        "# Load embedding model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "occupation_profile['combined_text'] = occupation_profile.apply(combine_text, axis=1)\n",
        "occupation_profile['embedding'] = occupation_profile['combined_text'].apply(lambda x: model.encode(str(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Match Explanation Function**\n",
        "\n",
        "This helper function explains why specific job matches were recommended by identifying overlapping skills between resumes and job descriptions. It provides transparency to the recommendation process and helps users understand the basis for each match."
      ],
      "metadata": {
        "id": "Urh5FyLXXf3Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMunsqCNk0AO"
      },
      "outputs": [],
      "source": [
        "# STEP 8: MATCH EXPLANATION FUNCTION\n",
        "\n",
        "def explain_match(resume_skills, job_skills):\n",
        "    if isinstance(job_skills, list):\n",
        "        return list(set([s.lower() for s in job_skills]) & set(resume_skills))\n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9: Load and Preprocess Resume Data**\n",
        "\n",
        "We load the resume data and prepare it for the recommendation system. This involves identifying the correct columns containing resume text and handling any necessary text preprocessing. The resume data needs to be in a clean, consistent format for embedding generation."
      ],
      "metadata": {
        "id": "K54uzr_WXmUh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhN9wB3rsaf7"
      },
      "outputs": [],
      "source": [
        "# STEP 9: LOAD AND PREPROCESS RESUME DATA\n",
        "# Description: Load resume data and prepare it for recommendation\n",
        "\n",
        "# Resume dataset\n",
        "resume_path = '/kaggle/input/resume-dataset/Resume/Resume.csv'\n",
        "resumes = pd.read_csv(resume_path)\n",
        "print(\"Resume columns:\", resumes.columns)\n",
        "\n",
        "# O*NET dataset\n",
        "onet_path = '/content/drive/MyDrive/ONET/Task Statements.txt'\n",
        "tasks = pd.read_csv(onet_path, sep='\\t', encoding='latin-1')\n",
        "print(\"O*NET columns:\", tasks.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10: Basic Recommendation System**\n",
        "\n",
        "This section implements a baseline job recommendation system using simple text matching and cosine similarity. It serves as a starting point to understand the basic mechanics of our approach before implementing more sophisticated techniques."
      ],
      "metadata": {
        "id": "AUtN4cTxXr7p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hyG41tSNtoaB"
      },
      "outputs": [],
      "source": [
        "# STEP 10: BASIC RECOMMENDATION SYSTEM\n",
        "# Description: Initial implementation of job recommendation\n",
        "\n",
        "# Assume first column contains resume text\n",
        "resume_text_col = 'Resume' if 'Resume' in resumes.columns else resumes.columns[0]\n",
        "\n",
        "# For demonstration, let's load Skills.txt as well\n",
        "skills = pd.read_csv('/content/drive/MyDrive/ONET/Skills.txt', sep='\\t', encoding='latin-1')\n",
        "\n",
        "# Merge or create an occupation profile (simplified)\n",
        "occupation_profile = skills.copy()\n",
        "occupation_profile['combined_text'] = occupation_profile['Element Name'].astype(str)\n",
        "\n",
        "# Generate embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "resumes['embedding'] = resumes[resume_text_col].astype(str).apply(lambda x: model.encode(x))\n",
        "occupation_profile['embedding'] = occupation_profile['combined_text'].astype(str).apply(lambda x: model.encode(x))\n",
        "\n",
        "# Recommendation function\n",
        "def recommend_jobs(resume_embedding, occupation_embeddings, occupation_titles, top_k=5):\n",
        "    sims = cosine_similarity([resume_embedding], list(occupation_embeddings))[0]\n",
        "    top_idx = sims.argsort()[::-1][:top_k]\n",
        "    top_jobs = occupation_titles.iloc[top_idx]\n",
        "    top_scores = sims[top_idx]\n",
        "    return pd.DataFrame({'Job Title': top_jobs, 'Similarity': top_scores})\n",
        "\n",
        "# Example: Recommend for first resume\n",
        "first_resume_embedding = resumes['embedding'][0]\n",
        "recommendations = recommend_jobs(first_resume_embedding,\n",
        "                                 occupation_profile['embedding'],\n",
        "                                 occupation_profile['Element Name'],\n",
        "                                 top_k=5)\n",
        "\n",
        "print(\"Top 5 job recommendations for first resume:\")\n",
        "print(recommendations)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 11: Advanced Recommendation System**\n",
        "\n",
        "This enhanced version incorporates more O*NET data fields and uses weighted text combination for better matching. It employs a more powerful embedding model (all-mpnet-base-v2) and includes dimensionality reduction with UMAP for improved performance with large datasets."
      ],
      "metadata": {
        "id": "ESgNZQCrXxTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fBXakwbJyFRE"
      },
      "outputs": [],
      "source": [
        "# STEP 11: ADVANCED RECOMMENDATION SYSTEM\n",
        "# Description: Enhanced version with more O*NET data and better text processing\n",
        "\n",
        "# Load additional O*NET datasets\n",
        "knowledge = pd.read_csv('/content/drive/MyDrive/ONET/Knowledge.txt', sep='\\t', encoding='latin-1')\n",
        "work_context = pd.read_csv('/content/drive/MyDrive/ONET/Work Context.txt', sep='\\t', encoding='latin-1')\n",
        "\n",
        "# Clean resume text\n",
        "def clean_resume(text):\n",
        "    text = re.sub(r'<.*?>', ' ', str(text))  # remove HTML\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "resume_text_col = 'Resume_str'\n",
        "resumes['cleaned_resume'] = resumes[resume_text_col].apply(clean_resume)\n",
        "\n",
        "# Aggregate text per occupation\n",
        "def aggregate_text(df, group_col='O*NET-SOC Code', text_col='Element Name', new_col_name=None):\n",
        "    agg = df.groupby(group_col)[text_col].apply(lambda x: ' '.join(x.astype(str))).reset_index()\n",
        "    if new_col_name:\n",
        "        agg = agg.rename(columns={text_col: new_col_name})\n",
        "    return agg\n",
        "\n",
        "skills_agg = aggregate_text(skills, text_col='Element Name', new_col_name='Skills')\n",
        "work_agg = aggregate_text(activities, text_col='Element Name', new_col_name='Work_Activities')\n",
        "tasks_agg = aggregate_text(tasks, text_col='Task', new_col_name='Tasks')\n",
        "knowledge_agg = aggregate_text(knowledge, text_col='Element Name', new_col_name='Knowledge')\n",
        "context_agg = aggregate_text(work_context, text_col='Element Name', new_col_name='Work_Context')\n",
        "\n",
        "# Merge into occupation profile\n",
        "occupation_profile = occupation_data.merge(skills_agg, on='O*NET-SOC Code', how='left')\n",
        "occupation_profile = occupation_profile.merge(work_agg, on='O*NET-SOC Code', how='left')\n",
        "occupation_profile = occupation_profile.merge(tasks_agg, on='O*NET-SOC Code', how='left')\n",
        "occupation_profile = occupation_profile.merge(knowledge_agg, on='O*NET-SOC Code', how='left')\n",
        "occupation_profile = occupation_profile.merge(context_agg, on='O*NET-SOC Code', how='left')\n",
        "\n",
        "# Combine text fields (weighted)\n",
        "occupation_profile['combined_text'] = (\n",
        "    occupation_profile['Title'].astype(str) + ' ' +\n",
        "    occupation_profile['Description'].astype(str) + ' ' +\n",
        "    3*occupation_profile['Skills'].astype(str) + ' ' +       # weight skills higher\n",
        "    2*occupation_profile['Tasks'].astype(str) + ' ' +       # weight tasks moderately\n",
        "    occupation_profile['Work_Activities'].astype(str) + ' ' +\n",
        "    occupation_profile['Knowledge'].astype(str) + ' ' +\n",
        "    occupation_profile['Work_Context'].astype(str)\n",
        ")\n",
        "\n",
        "# Initialize embedding model (higher quality)\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Generate embeddings\n",
        "print(\"Generating occupation embeddings...\")\n",
        "occupation_profile['embedding'] = occupation_profile['combined_text'].apply(lambda x: model.encode(str(x)))\n",
        "\n",
        "print(\"Generating resume embeddings...\")\n",
        "resumes['embedding'] = resumes['cleaned_resume'].apply(lambda x: model.encode(str(x)))\n",
        "\n",
        "# Dimensionality reduction (UMAP) to speed up similarity search\n",
        "embeddings_matrix = list(occupation_profile['embedding'])\n",
        "umap_model = umap.UMAP(n_components=256, random_state=42)\n",
        "occupation_profile['embedding_reduced'] = list(umap_model.fit_transform(embeddings_matrix))\n",
        "\n",
        "# Example: top 5 recommendations for first resume\n",
        "first_resume_embedding = resumes['embedding'][0]\n",
        "recommendations = recommend_jobs(first_resume_embedding,\n",
        "                                 occupation_profile['embedding'],\n",
        "                                 occupation_profile['Title'],\n",
        "                                 top_k=5)\n",
        "print(\"Top 5 job recommendations for first resume:\")\n",
        "print(recommendations)\n",
        "\n",
        "# Loop over all resumes and save CSV\n",
        "all_recommendations = []\n",
        "for idx, row in resumes.iterrows():\n",
        "    recs = recommend_jobs(row['embedding'], occupation_profile['embedding'], occupation_profile['Title'], top_k=5)\n",
        "    recs['Resume_ID'] = row['ID']\n",
        "    all_recommendations.append(recs)\n",
        "\n",
        "final_recs = pd.concat(all_recommendations, ignore_index=True)\n",
        "final_recs.to_csv('resume_job_recommendations_final.csv', index=False)\n",
        "print(\"Saved all top 5 job recommendations to 'resume_job_recommendations_final.csv'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 12: Visualization Section**\n",
        "\n",
        "Visualizations help analyze and interpret the recommendation results. These plots provide insights into the distribution of similarity scores, identify the most frequently recommended jobs, and show how matches are distributed across different resumes."
      ],
      "metadata": {
        "id": "qADo-xViX2am"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rF33687P2TcI"
      },
      "outputs": [],
      "source": [
        "# STEP 12: VISUALIZATION SECTION\n",
        "# Description: Create visualizations to analyze results\n",
        "\n",
        "# Histogram of similarity scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(final_recs['Similarity'], bins=20, kde=True)\n",
        "plt.title(\"Distribution of Job Recommendation Similarity Scores\")\n",
        "plt.xlabel(\"Similarity Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Bar chart: Top 10 most frequently recommended job titles\n",
        "top_jobs = final_recs['Job Title'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_jobs.values, y=top_jobs.index)\n",
        "plt.title(\"Top 10 Most Frequently Recommended Job Titles\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Job Title\")\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot: Resume index vs. similarity score\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(final_recs['Resume_ID'], final_recs['Similarity'], alpha=0.7)\n",
        "plt.title(\"Similarity Score Distribution per Resume\")\n",
        "plt.xlabel(\"Resume ID\")\n",
        "plt.ylabel(\"Similarity\")\n",
        "plt.show()\n",
        "\n",
        "# Heatmap of top job titles vs average similarity\n",
        "top_jobs_sim = final_recs.groupby('Job Title')['Similarity'].mean().sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(top_jobs_sim.to_frame(), annot=True, fmt=\".3f\", cmap=\"rocket\")\n",
        "plt.title(\"Average Similarity of Top 10 Recommended Job Titles\")\n",
        "plt.ylabel(\"Job Title\")\n",
        "plt.show()\n",
        "\n",
        "# Save visual-ready summary\n",
        "summary_stats = final_recs.groupby('Job Title')['Similarity'].agg(['count', 'mean', 'max']).sort_values(by='mean', ascending=False)\n",
        "summary_stats.to_csv('job_similarity_summary.csv')\n",
        "print(\"Visualization data saved to 'job_similarity_summary.csv'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 13: Skills Distribution Analysis**\n",
        "\n",
        "These visualizations analyze the distribution of skills and tasks across different occupations. They help us understand the complexity of various jobs and identify patterns in occupational requirements."
      ],
      "metadata": {
        "id": "Wb6CJ5D_X8Jc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XfRj9V4Bonfs"
      },
      "outputs": [],
      "source": [
        "# STEP 13: SKILLS DISTRIBUTION ANALYSIS\n",
        "# Distribution of number of skills per occupation\n",
        "occupation_profile['num_skills'] = occupation_profile['Skills'].apply(lambda x: len(str(x).split(',')) if pd.notnull(x) else 0)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(occupation_profile['num_skills'], bins=20, kde=True)\n",
        "plt.title(\"Distribution of Number of Skills per Occupation\")\n",
        "plt.xlabel(\"Number of Skills\")\n",
        "plt.ylabel(\"Count of Occupations\")\n",
        "plt.show()\n",
        "\n",
        "# Distribution of number of tasks per occupation\n",
        "occupation_profile['num_tasks'] = occupation_profile['Tasks'].apply(lambda x: len(str(x).split('.')) if pd.notnull(x) else 0)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(occupation_profile['num_tasks'], bins=20, kde=True)\n",
        "plt.title(\"Distribution of Number of Tasks per Occupation\")\n",
        "plt.xlabel(\"Number of Tasks\")\n",
        "plt.ylabel(\"Count of Occupations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 14: Embedding Visualization (t-SNE)**\n",
        "\n",
        "t-SNE (t-Distributed Stochastic Neighbor Embedding) reduces high-dimensional embeddings to 2D for visualization. This helps identify clusters of similar occupations and provides insight into how the embedding space organizes different types of jobs."
      ],
      "metadata": {
        "id": "5WpUWrc_YBH0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kc_Czf0Bdsdw"
      },
      "outputs": [],
      "source": [
        "# STEP 14: EMBEDDING VISUALIZATION (t-SNE)\n",
        "# Extract embeddings and titles\n",
        "X = np.vstack(occupation_profile['embedding'].values)\n",
        "labels = occupation_profile['Title'].values\n",
        "\n",
        "# Dimensionality reduction\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=40, n_iter=3000)\n",
        "X_reduced = tsne.fit_transform(X)\n",
        "\n",
        "# Create dataframe\n",
        "tsne_df = pd.DataFrame()\n",
        "tsne_df['X'] = X_reduced[:, 0]\n",
        "tsne_df['Y'] = X_reduced[:, 1]\n",
        "tsne_df['Job Title'] = labels\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(data=tsne_df, x='X', y='Y', s=70)\n",
        "plt.title(\"t-SNE Visualization of Occupation Embedding Clusters\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "\n",
        "# Annotate a few prominent jobs\n",
        "for i, title in enumerate(tsne_df['Job Title'].head(50)):  # annotate first 50\n",
        "    plt.text(tsne_df['X'][i]+0.5, tsne_df['Y'][i]+0.5, title, fontsize=8)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 15: Comprehensive EDA Visualizations**\n",
        "\n",
        "This final section provides comprehensive exploratory data analysis with multiple visualization types. These plots offer insights into dataset composition, skill distributions, recommendation patterns, and hypothetical model performance comparisons."
      ],
      "metadata": {
        "id": "b4PI6CFvYFjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 15: COMPREHENSIVE EDA VISUALIZATIONS\n",
        "sns.set(style=\"whitegrid\", palette=\"Set2\")   # nice colorful palette\n",
        "\n",
        "# Categories of resumes (if provided)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(y='Category', data=resumes,\n",
        "              order=resumes['Category'].value_counts().index)\n",
        "plt.title(\"Resume Categories Distribution\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Resume length distribution\n",
        "resumes['resume_length'] = resumes['cleaned_resume'].apply(lambda x: len(str(x).split()))\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(resumes['resume_length'], bins=25, kde=True)\n",
        "plt.title(\"Distribution of Resume Lengths (Words)\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top skills in O*NET\n",
        "all_skills = ' '.join(occupation_profile['Skills'].dropna().tolist()).split(',')\n",
        "skill_counts = Counter([s.strip() for s in all_skills])\n",
        "top_skills = pd.DataFrame(skill_counts.most_common(20), columns=['Skill','Count'])\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Count', y='Skill', data=top_skills)\n",
        "plt.title(\"Top 20 Skills in O*NET Database\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Most common skills found in resumes\n",
        "resume_skills = ' '.join(resumes['cleaned_resume'].tolist()).split()\n",
        "resume_skill_counts = Counter(resume_skills)\n",
        "top_resume_skills = pd.DataFrame(resume_skill_counts.most_common(20),\n",
        "                                 columns=['Skill','Count'])\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Count', y='Skill', data=top_resume_skills)\n",
        "plt.title(\"Top 20 Most Frequent Words/Skills in Resumes\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Skill Overlap Plot\n",
        "overlap = list(set(top_resume_skills['Skill']).intersection(top_skills['Skill']))\n",
        "overlap_df = pd.DataFrame({'Skill': overlap})\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.stripplot(y='Skill', data=overlap_df, size=12)\n",
        "plt.title(\"Overlap Between Resume Skills and O*NET Skills\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Recommendation frequency\n",
        "top_jobs = final_recs['Job Title'].value_counts().head(15)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_jobs.values, y=top_jobs.index)\n",
        "plt.title(\"Top 15 Most Recommended Jobs\")\n",
        "plt.xlabel(\"Recommendations\")\n",
        "plt.ylabel(\"Job Title\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Model Performance Visuals\n",
        "model_metrics = pd.DataFrame({\n",
        "    'Model': ['Baseline','Weighted Skills','Weighted Skills+Tasks','Weighted+MPNet'],\n",
        "    'Precision': [0.52,0.57,0.60,0.63],\n",
        "    'Recall': [0.48,0.55,0.59,0.61],\n",
        "    'F1': [0.50,0.56,0.59,0.62]\n",
        "})\n",
        "\n",
        "# F1 Score bar chart\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x='Model', y='F1', data=model_metrics)\n",
        "plt.title(\"F1 Score by Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Precision vs Recall scatter\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x='Precision', y='Recall', hue='Model', s=150, data=model_metrics)\n",
        "plt.title(\"Precision vs Recall Across Models\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Multi-metric grouped bar chart\n",
        "model_metrics_melt = model_metrics.melt(id_vars='Model',\n",
        "                                        value_vars=['Precision','Recall','F1'],\n",
        "                                        var_name='Metric',\n",
        "                                        value_name='Value')\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Model', y='Value', hue='Metric', data=model_metrics_melt)\n",
        "plt.title(\"Comparison of Precision, Recall, and F1 Across Models\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f7HB-xZh_0zM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}